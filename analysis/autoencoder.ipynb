{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from utils_analysis import normalize_dataframe2\n",
    "from math import ceil\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                          int64\n",
      "id                                  int64\n",
      "room_type                           int64\n",
      "price                               int64\n",
      "minimum_nights_x                    int64\n",
      "number_of_reviews                   int64\n",
      "reviews_per_month                 float64\n",
      "calculated_host_listings_count      int64\n",
      "host_response_time                  int64\n",
      "host_response_rate                  int64\n",
      "property_type                       int64\n",
      "accommodates                        int64\n",
      "bathrooms                         float64\n",
      "bedrooms                          float64\n",
      "beds                              float64\n",
      "bed_type                            int64\n",
      "cleaning_fee                      float64\n",
      "minimum_nights_y                    int64\n",
      "maximum_nights                      int64\n",
      "minimum_minimum_nights              int64\n",
      "maximum_minimum_nights              int64\n",
      "minimum_maximum_nights              int64\n",
      "maximum_maximum_nights              int64\n",
      "review_scores_rating              float64\n",
      "review_scores_accuracy            float64\n",
      "review_scores_cleanliness         float64\n",
      "review_scores_checkin             float64\n",
      "review_scores_communication       float64\n",
      "review_scores_location            float64\n",
      "review_scores_value               float64\n",
      "cancellation_policy                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"caracteristicas.csv\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                          int64\n",
      "room_type                           int64\n",
      "price                               int64\n",
      "minimum_nights_x                    int64\n",
      "number_of_reviews                   int64\n",
      "reviews_per_month                 float64\n",
      "calculated_host_listings_count      int64\n",
      "host_response_time                  int64\n",
      "host_response_rate                  int64\n",
      "property_type                       int64\n",
      "accommodates                        int64\n",
      "bathrooms                         float64\n",
      "bedrooms                          float64\n",
      "beds                              float64\n",
      "bed_type                            int64\n",
      "cleaning_fee                      float64\n",
      "minimum_nights_y                    int64\n",
      "maximum_nights                      int64\n",
      "minimum_minimum_nights              int64\n",
      "maximum_minimum_nights              int64\n",
      "minimum_maximum_nights              int64\n",
      "maximum_maximum_nights              int64\n",
      "review_scores_rating              float64\n",
      "review_scores_accuracy            float64\n",
      "review_scores_cleanliness         float64\n",
      "review_scores_checkin             float64\n",
      "review_scores_communication       float64\n",
      "review_scores_location            float64\n",
      "review_scores_value               float64\n",
      "cancellation_policy                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = data.drop(labels=['id'], axis=1)\n",
    "# data = data[['price', 'number_of_reviews']]\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:  (25132, 30)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# data, scaler = normalize_dataframe2(data)\n",
    "data = data.to_numpy().astype('float32')\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "\n",
    "print(\"Shape of data: \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "input_dim = data.shape[1]\n",
    "n_hidden_1 = 31\n",
    "n_hidden_2 = 31\n",
    "n_hidden_3 = 31\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_steps = 100000\n",
    "batch_size = 10\n",
    "display_step = 1000\n",
    "test_size = 1000\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=1000)\n",
    "\n",
    "test_dataset = dataset.take(test_size)\n",
    "train_dataset = dataset.skip(test_size)\n",
    "\n",
    "n_repeat = ceil(num_steps * batch_size /( data.shape[0] - test_size))\n",
    "train_dataset = train_dataset.repeat(n_repeat)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "iter = train_dataset.make_one_shot_iterator()\n",
    "X = iter.get_next()\n",
    "\n",
    "print(n_repeat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/texs/.virtualenvs/env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weigths = {\n",
    "    'encoder_h1' : tf.Variable(tf.random_normal([input_dim, n_hidden_1])),\n",
    "    'encoder_h2' : tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'decoder_h1' : tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'decoder_h2' : tf.Variable(tf.random_normal([n_hidden_3, input_dim]))    \n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_h1' : tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_h2' : tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_h1' : tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'decoder_h2' : tf.Variable(tf.random_normal([input_dim]))    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_encoder(x):\n",
    "    l1 = tf.nn.relu(tf.add(tf.matmul(x, weigths['encoder_h1']), biases['encoder_h1']))\n",
    "    l2 = tf.nn.sigmoid(tf.add(tf.matmul(l1, weigths['encoder_h2']), biases['encoder_h2']))\n",
    "    return l2\n",
    "\n",
    "def red_decoder(x):\n",
    "    l1 = tf.nn.relu(tf.add(tf.matmul(x, weigths['decoder_h1']), biases['decoder_h1']))\n",
    "    l2 = tf.nn.relu(tf.add(tf.matmul(l1, weigths['decoder_h2']), biases['decoder_h2']))\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = red_encoder(X)\n",
    "decoder = red_decoder(encoder)\n",
    "\n",
    "y_true = X\n",
    "y_pred = decoder\n",
    "\n",
    "loss = tf.reduce_sum(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "optimizer = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Minibatch Loss: 4495960.000000\n",
      "Step 1000: Minibatch Loss: 3966454.400000\n",
      "Step 2000: Minibatch Loss: 4301632.400000\n",
      "Step 3000: Minibatch Loss: 12340022.400000\n",
      "Step 4000: Minibatch Loss: 6400575.200000\n",
      "Step 5000: Minibatch Loss: 983184793.600000\n",
      "Step 6000: Minibatch Loss: 5565180.400000\n",
      "Step 7000: Minibatch Loss: 2897680.600000\n",
      "Step 8000: Minibatch Loss: 10424424.000000\n",
      "Step 9000: Minibatch Loss: 4239021.200000\n",
      "Step 10000: Minibatch Loss: 169056000.000000\n",
      "Step 11000: Minibatch Loss: 14423502.400000\n",
      "Step 12000: Minibatch Loss: 5252931.200000\n",
      "Step 13000: Minibatch Loss: 5476952.000000\n",
      "Step 14000: Minibatch Loss: 8826670.400000\n",
      "Step 15000: Minibatch Loss: 20395947.200000\n",
      "Step 16000: Minibatch Loss: 2898830.000000\n",
      "Step 17000: Minibatch Loss: 1785059328.000000\n",
      "Step 18000: Minibatch Loss: 6310416.400000\n",
      "Step 19000: Minibatch Loss: 2312229.600000\n",
      "Step 20000: Minibatch Loss: 8955646.400000\n",
      "Step 21000: Minibatch Loss: 4738771.200000\n",
      "Step 22000: Minibatch Loss: 350724684.800000\n",
      "Step 23000: Minibatch Loss: 4110719.600000\n",
      "Step 24000: Minibatch Loss: 7678914.400000\n",
      "Step 25000: Minibatch Loss: 47242489.600000\n",
      "Step 26000: Minibatch Loss: 2897523.600000\n",
      "Step 27000: Minibatch Loss: 290484480.000000\n",
      "Step 28000: Minibatch Loss: 2285859.600000\n",
      "Step 29000: Minibatch Loss: 2500318208.000000\n",
      "Step 30000: Minibatch Loss: 3295381.200000\n",
      "Step 31000: Minibatch Loss: 4143061.200000\n",
      "Step 32000: Minibatch Loss: 5437457.600000\n",
      "Step 33000: Minibatch Loss: 7057964.800000\n",
      "Step 34000: Minibatch Loss: 1216967270.400000\n",
      "Step 35000: Minibatch Loss: 10442786.400000\n",
      "Step 36000: Minibatch Loss: 2359809.200000\n",
      "Step 37000: Minibatch Loss: 7878692.000000\n",
      "Step 38000: Minibatch Loss: 2691139.200000\n",
      "Step 39000: Minibatch Loss: 444218470.400000\n",
      "Step 40000: Minibatch Loss: 3264030.000000\n",
      "Step 41000: Minibatch Loss: 5787500.800000\n",
      "Step 42000: Minibatch Loss: 13884841.600000\n",
      "Step 43000: Minibatch Loss: 2655691.800000\n",
      "Step 44000: Minibatch Loss: 64415744.000000\n",
      "Step 45000: Minibatch Loss: 2985109.800000\n",
      "Step 46000: Minibatch Loss: 1673650585.600000\n",
      "Step 47000: Minibatch Loss: 8805304.000000\n",
      "Step 48000: Minibatch Loss: 2513452.200000\n",
      "Step 49000: Minibatch Loss: 8823453.600000\n",
      "Step 50000: Minibatch Loss: 3033977.200000\n",
      "Step 51000: Minibatch Loss: 746633932.800000\n",
      "Step 52000: Minibatch Loss: 3475310.000000\n",
      "Step 53000: Minibatch Loss: 1951270.400000\n",
      "Step 54000: Minibatch Loss: 6751585.600000\n",
      "Step 55000: Minibatch Loss: 3014114.000000\n",
      "Step 56000: Minibatch Loss: 245758284.800000\n",
      "Step 57000: Minibatch Loss: 4043947.200000\n",
      "Step 58000: Minibatch Loss: 2129389977.600000\n",
      "Step 59000: Minibatch Loss: 7563408.000000\n",
      "Step 60000: Minibatch Loss: 3618527.600000\n",
      "Step 61000: Minibatch Loss: 9868635.200000\n",
      "Step 62000: Minibatch Loss: 3014146.200000\n",
      "Step 63000: Minibatch Loss: 1096819200.000000\n",
      "Step 64000: Minibatch Loss: 5739012.800000\n",
      "Step 65000: Minibatch Loss: 2403671.400000\n",
      "Step 66000: Minibatch Loss: 6332832.000000\n",
      "Step 67000: Minibatch Loss: 14570707.200000\n",
      "Step 68000: Minibatch Loss: 317728025.600000\n",
      "Step 69000: Minibatch Loss: 3313723.400000\n",
      "Step 70000: Minibatch Loss: 2652925542.400000\n",
      "Step 71000: Minibatch Loss: 8776710.400000\n",
      "Step 72000: Minibatch Loss: 1915024.200000\n",
      "Step 73000: Minibatch Loss: 45372451.200000\n",
      "Step 74000: Minibatch Loss: 4096166.800000\n",
      "Step 75000: Minibatch Loss: 1425821900.800000\n",
      "Step 76000: Minibatch Loss: 4541815.200000\n",
      "Step 77000: Minibatch Loss: 6043530.000000\n",
      "Step 78000: Minibatch Loss: 6728961.600000\n",
      "Step 79000: Minibatch Loss: 4325764.400000\n",
      "Step 80000: Minibatch Loss: 591854182.400000\n",
      "Step 81000: Minibatch Loss: 4368408.800000\n",
      "Step 82000: Minibatch Loss: 5215616.000000\n",
      "Step 83000: Minibatch Loss: 7142964.800000\n",
      "Step 84000: Minibatch Loss: 1970397.000000\n",
      "Step 85000: Minibatch Loss: 121926208.000000\n",
      "Step 86000: Minibatch Loss: 5353770.000000\n",
      "Step 87000: Minibatch Loss: 1933884620.800000\n",
      "Step 88000: Minibatch Loss: 4938670.000000\n",
      "Step 89000: Minibatch Loss: 5220868.400000\n",
      "Step 90000: Minibatch Loss: 27223811.200000\n",
      "Step 91000: Minibatch Loss: 12907771.200000\n",
      "Step 92000: Minibatch Loss: 783276083.200000\n",
      "Step 93000: Minibatch Loss: 3513400.400000\n",
      "Step 94000: Minibatch Loss: 7798137.600000\n",
      "Step 95000: Minibatch Loss: 10076504.000000\n",
      "Step 96000: Minibatch Loss: 3301208.600000\n",
      "Step 97000: Minibatch Loss: 184921395.200000\n",
      "Step 98000: Minibatch Loss: 4687901.600000\n",
      "Step 99000: Minibatch Loss: 2285934182.400000\n",
      "Step 100000: Minibatch Loss: 4162739.200000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "for i in range(1, num_steps + 1):\n",
    "    _, l = sess.run([optimizer, loss])\n",
    "    if i % display_step == 0 or i == 1:\n",
    "        print('Step %i: Minibatch Loss: %f' % (i, l/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.batch(1)\n",
    "iter_test = test_dataset.make_one_shot_iterator()\n",
    "I = iter_test.get_next()\n",
    "\n",
    "# I = tf.placeholder(tf.float32, shape=([None, input_dim]))\n",
    "\n",
    "d_encoder = red_encoder(I)\n",
    "d_decoder = red_decoder(d_encoder)\n",
    "perdida = tf.reduce_sum(tf.pow(d_decoder - I, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30)\n",
      "789712800.0\n",
      "[[29579.445        0.         135.64523      0.           0.\n",
      "      0.           0.           0.          87.56758      0.\n",
      "      0.           0.           0.           0.           0.\n",
      "     48.167988     0.         632.5087       0.           0.\n",
      "    627.5918     627.57294      0.           0.           0.\n",
      "      0.           0.           0.           0.           0.      ]]\n",
      "[[1.485e+03 0.000e+00 1.700e+02 3.000e+01 2.900e+02 3.940e+00 1.000e+00\n",
      "  3.000e+00 1.000e+02 6.000e+00 2.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      "  4.000e+00 5.000e+01 3.000e+01 3.050e+02 3.000e+01 3.000e+01 3.050e+02\n",
      "  3.050e+02 9.900e+01 1.000e+01 1.000e+01 1.000e+01 1.000e+01 1.000e+01\n",
      "  1.000e+01 2.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "v, l, i = sess.run([d_decoder, perdida, I])\n",
    "print(v.shape)\n",
    "print(l)\n",
    "\n",
    "\n",
    "# i_r = scaler.inverse_transform(v)\n",
    "# o_r = scaler.inverse_transform(i)\n",
    "# print(i_r)\n",
    "# print(o_r)\n",
    "\n",
    "print(v)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.8094445e+04  0.0000000e+00 -3.4354767e+01 -3.0000000e+01\n",
      "  -2.9000000e+02 -3.9400001e+00 -1.0000000e+00 -3.0000000e+00\n",
      "  -1.2432419e+01 -6.0000000e+00 -2.0000000e+00 -1.0000000e+00\n",
      "  -1.0000000e+00 -1.0000000e+00 -4.0000000e+00 -1.8320122e+00\n",
      "  -3.0000000e+01  3.2750873e+02 -3.0000000e+01 -3.0000000e+01\n",
      "   3.2259180e+02  3.2257294e+02 -9.9000000e+01 -1.0000000e+01\n",
      "  -1.0000000e+01 -1.0000000e+01 -1.0000000e+01 -1.0000000e+01\n",
      "  -1.0000000e+01 -2.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(v - i)\n",
    "# print(i_r - o_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
